# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15zVCljMnWWthADrP8-VE1GWxFZWFh3Cq

### Importing the libraries
"""

import numpy as np
import pandas as pd
import requests
import seaborn as sns

import math
import random

import yfinance as yf
import matplotlib.pyplot as plt

import bs4 as bs
import datetime as dt
import os
import pickle
import requests

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import pandas_datareader as pdr
# %matplotlib inline

"""## Data Preprocessing"""

# Download the CSV file containing S&P 500 stock symbols
url = 'https://datahub.io/core/s-and-p-500-companies/r/constituents.csv'
sp500 = pd.read_csv(url)

sp500

df2 = sp500.pivot_table(index = ['Sector'], aggfunc ='size')
print(df2)

# create a list of sectors to include in the portfolio
sectors = ['Energy', 'Financials', 'Information Technology', 'Health Care']

# create a boolean mask for the tickers in the desired sectors
mask = sp500['Sector'].isin(sectors)

# filter the tickers using the boolean mask
portfolio = sp500[mask]

# Use yfinance to gather the pricing data, isolating the Closing prices
start_day = "2018-01-01"
end_day = "2023-04-10"

# Download the data for each ticker
bad_tickers = []
for ticker in portfolio.Symbol:
    if '.' in ticker:
        bad_tickers.append(ticker)
# Exclude the bad tickers from our list
portfolios = portfolio[~portfolio.Symbol.isin(bad_tickers)]['Symbol']
assets = {}
for ticker in portfolios:
    print(f'Downloading data for {ticker}...')
    ticker_data_fin = yf.download(ticker, start=start_day, end=end_day)
    assets[ticker] = ticker_data_fin[['Open', 'High', 'Low', 'Close', 'Volume']]

assets

# Drop the key-value pairs with empty DataFrames from the dictionary
assets = {k: v for k, v in assets.items() if not isinstance(v, pd.DataFrame) or not v.empty}

assets.keys()

#Detecting missing values
missing_tickers = []
for ticker, df in assets.items():
    if df.shape != (1325, 5):
      missing_tickers.append(ticker)
      
      print(f"{ticker} has shape {df.shape}")

# Drop tickers with missing values from the dictionary
for ticker in missing_tickers:
    assets.pop(ticker)

#del stocks['AMCR']

assets.keys()

stock_df = pd.concat(assets, axis=1, keys=assets.keys())
stock_df.columns.names = ['Ticker', 'Stock Info']

stock_df.columns.levels[0]

stock_df.shape

stock_df

price_cols = ['Open','High','Low','Close','Volume']

def calculate_pct_change(df, price_cols, period):
  
    pct_changes = {}
    
    # Loop over the stock tickers and calculate the percentage change for each price column
    for ticker in df.columns.levels[0]:
      
        stock_data = df[ticker]
        pct_changes[ticker] = pd.DataFrame()
        
        for col in price_cols:
            pct_changes[ticker][col] = stock_data[col].pct_change(periods=period)
            
    # Concatenate all the percentage changes for each stock and drop any rows with NaN values
    pct_changes_df = pd.concat(pct_changes.values(), axis=1, keys=df.columns.levels[0])
    pct_changes_df.columns.names = ['Ticker', 'Stock Info']
    pct_changes_df.dropna(thresh=1,inplace=True)
    
    return pct_changes_df

pct_changes_df=calculate_pct_change(stock_df, price_cols, 20)
pct_changes_df

df = pd.DataFrame(index=['dm','dmm'],
                  columns=pd.MultiIndex.from_product([pct_changes_df.columns.levels[0],
                                                      ['Open','High','Low','Close','Volume']]))

def calc_med(df,price_cols, med_data):
  for ticker in df.columns.levels[0]:
    for col_type in price_cols:
      col = df.loc[:, (ticker, col_type)]
      med_data.loc['dm', (ticker, col_type)] = np.median(col)
      med_data.loc['dmm', (ticker, col_type)] = np.median(np.abs(col - np.median(col)))
  return med_data

mediana_mad = calc_med(pct_changes_df,price_cols, df)

# plot close prices for each stock separately
def plot_med_mad(df, med_data,price_cols ):
  for stock in df.columns.levels[0]:
    fig, ax = plt.subplots(figsize=(10,6))
    col = df.loc[:, (stock, price_cols)]
    col.plot(label=stock, ax=ax,c='b')
    ax.axhline(y=med_data.loc[:, (stock, price_cols)][0], c='r', label='median')
    ax.axhline(y=5*med_data.loc[:, (stock, price_cols)][1], c='y', linestyle='-.', label='mad')
    ax.axhline(y=-5*med_data.loc[:, (stock, price_cols)][1], c='y', linestyle='-.', label='mad')
    ax.set_title(stock)
    ax.set_xlabel('Time')
    ax.set_ylabel(f'{price_cols} Price')
    ax.legend(loc='lower right')
    plt.show()

plot_med_mad(pct_changes_df,mediana_mad,'Close')

pct_changes_df.shape

outliers = pd.DataFrame(index=pct_changes_df.index,
                  columns=pd.MultiIndex.from_product([pct_changes_df.columns.levels[0],
                                                      ['Open','High','Low','Close','Volume']]))

def outliers_detection(df, price_cols, med_data, outliers):
    """
    Detect and modify outliers in a DataFrame containing stock price data.
    
    Args:
        df (pd.DataFrame): Input DataFrame containing stock price data.
        price_cols (list): List of column names in df that represent the price data.
        med_data (pd.DataFrame): DataFrame containing median values for each stock and price column.
        outliers (pd.DataFrame): DataFrame to store the modified values for the detected outliers.
        
    Returns:
        pd.DataFrame: DataFrame with modified values for the detected outliers.
    """
    for stock in df.columns.levels[0]:
        for col_type in price_cols:
            col = df.loc[:, (stock, col_type)]
            mcol = med_data.loc[:, (stock, col_type)]
            
            # Calculate median absolute deviation (MAD)
            mad = np.median(np.abs(col - mcol[0]))
            
            # Detect and modify outliers
            upper_threshold = mcol[0] + 5 * mad
            lower_threshold = mcol[0] - 5 * mad
            col = np.where(col > upper_threshold, upper_threshold, col)
            col = np.where(col < lower_threshold, lower_threshold, col)
            
            # Store modified values in outliers DataFrame
            outliers.loc[:, (stock, col_type)] = col
    
    return outliers

outliers = outliers_detection(pct_changes_df,price_cols,mediana_mad,outliers)

"""Splitting the data"""

# Filter data for training, validation, and test sets
train_data_x = outliers.loc[(outliers.index >= '2018-01-31') & (outliers.index <= '2020-12-31')]
val_data_x = outliers.loc[(outliers.index >= '2021-01-01') & (outliers.index <= '2021-12-31')]
test_data_x = outliers.loc[(outliers.index >= '2022-01-01') & (outliers.index <= '2023-12-31')]

print(train_data_x.shape,val_data_x.shape,test_data_x.shape)

for i in outliers.columns.levels[0]:
  plt.figure(figsize=(14,4))
  plt.plot(train_data_x.loc[:, (i, 'Close')])
  plt.plot(val_data_x.loc[:, (i, 'Close')])
  plt.plot(test_data_x.loc[:, (i, 'Close')])
  plt.ylabel("Price")
  plt.xlabel("Date")
  plt.legend(["Training Set", "Validation Set","Test Set"])
  plt.title(i + " Closing Stock Price")

"""Scaling of dependent variables"""

from sklearn.preprocessing import StandardScaler
# Scaling the training, validation, and test sets
transform_train_x = {}
transform_val_x = {}
transform_test_x = {}


scaler = {}

for i in outliers.columns.levels[0]:
    sc =StandardScaler()
    tr = np.array(train_data_x[i])
    vl = np.array(val_data_x[i])
    ts = np.array(test_data_x[i])
    tr0 = tr.reshape(tr.shape[0],5)
    vl0 = vl.reshape(vl.shape[0],5)
    ts0 = ts.reshape(ts.shape[0],5)
    transform_train_x[i] = sc.fit_transform(tr0)
    transform_val_x[i] = sc.transform(vl0)
    transform_test_x[i] = sc.transform(ts0)

 

    scaler[i] = sc
del tr
del vl
del ts

trainset = {}
valset = {}
testset = {}
window_size = 1  # Updated window size

for j in outliers.columns.levels[0]:
    trainset[j] = {}
    X_train = []
    y_train = []
    for i in range(window_size, 736):
        X_train.append(transform_train_x[j][i-window_size:i])
        y_train.append(transform_train_x[j][i][3])  # Update indexing for train_y
    X_train, y_train = np.array(X_train), np.array(y_train)
    trainset[j]["X"] = np.reshape(X_train, (X_train.shape[0], window_size, 5))
    trainset[j]["y"] = y_train

    valset[j] = {}
    X_val = []
    y_val = []
    for i in range(window_size, 252):  
        X_val.append(transform_val_x[j][i-window_size:i])
        y_val.append(transform_val_x[j][i][3])  # Update indexing for val_y
    X_val, y_val = np.array(X_val), np.array(y_val)
    valset[j]["X"] = np.reshape(X_val, (X_val.shape[0], window_size, 5))
    valset[j]["y"] = y_val

    testset[j] = {}
    X_test = []
    y_test = []
    for i in range(window_size, 317):  
        X_test.append(transform_test_x[j][i-window_size:i])
        y_test.append(transform_test_x[j][i][3])
    X_test, y_test = np.array(X_test), np.array(y_test)
    testset[j]["X"] = np.reshape(X_test, (X_test.shape[0], window_size, 5))
    testset[j]["y"] = y_test

arr_buff = []
for i in outliers.columns.levels[0]:
    buff = {}
    buff["X_train"] = trainset[i]["X"].shape
    buff["y_train"] = trainset[i]["y"].shape
    buff["X_val"] = valset[i]["X"].shape
    buff["y_val"] = valset[i]["y"].shape
    buff["X_test"] = testset[i]["X"].shape
    buff["y_test"] = testset[i]["y"].shape
    arr_buff.append(buff)

pd.DataFrame(arr_buff, index=outliers.columns.levels[0])

trainset[i]["X"].shape

"""## LSTM """

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import RMSprop
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam
import tensorflow as tf
import keras

# Define hyperparameters
hidden_nodes = 5
hidden_layers = 1
learning_rate = 0.001
patient = 0
batch_size = 64
dropout_rate = 0.1
recurrent_dropout_rate = 0.2
optimizer = RMSprop()

# Build LSTM model
lstm = Sequential()
lstm.add(LSTM(hidden_nodes, input_shape=(trainset[i]["X"].shape[1], trainset[i]["X"].shape[2])))
lstm.add(Dropout(dropout_rate))
for _ in range(hidden_layers-1):
    lstm.add(LSTM(hidden_nodes))
    lstm.add(Dropout(dropout_rate))
lstm.add(Dense(1, activation='linear'))  # Change activation function to 'linear'
lstm.compile(loss='mean_squared_error', optimizer=optimizer)


# Train the model with early stopping for each dataset
histories = {}
for i in outliers.columns.levels[0]:
    print("Fitting to", i)
    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patient)
    history = lstm.fit(trainset[i]["X"], trainset[i]["y"], batch_size=batch_size, epochs=100, validation_data = (valset[i]["X"], valset[i]["y"]), callbacks=[early_stopping], verbose=1)
    histories[i] = history

# Create a dictionary to store average returns for each stock using a dictionary comprehension
returns = {i: lstm.predict(testset[i]["X"], batch_size=batch_size) for i in outliers.columns.levels[0]}

# Plot predicted results
for i in outliers.columns.levels[0]:
    # Plot actual vs. predicted values
    plt.figure(figsize=(12, 6))
    plt.plot(testset[i]["y"], label='Actual')
    plt.plot(returns[i], label='Predicted')
    plt.title('Actual vs. Predicted Close Price for ' + i)
    plt.xlabel('Time Step')
    plt.ylabel('Close Price')
    plt.legend()
    plt.show()

"""## **CNN**"""

from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from keras.optimizers import SGD

# Define CNN model
# Define CNN model
model = Sequential()
model.add(Conv1D(filters=32, kernel_size=1, activation='relu', input_shape=(1, 5)))
model.add(MaxPooling1D(pool_size=1))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(1))

# Compile model
model.compile(optimizer='adam', loss='mse')


# Train the model with early stopping for each dataset
histories_cnn = {}
for i in outliers.columns.levels[0]:
    print("Fitting to", i)
    early_stopping = EarlyStopping(monitor='val_loss', patience=10)
    history_cnn = model.fit(trainset[i]["X"], trainset[i]["y"], validation_data=(valset[i]["X"], valset[i]["y"]), epochs=100, batch_size=batch_size, callbacks=[early_stopping])
    histories_cnn[i] = history_cnn

returns_cnn = {}
# Loop through stocks
for i in outliers.columns.levels[0]:
    # Get predicted values
    y_pred_test_cnn = model.predict(testset[i]["X"], batch_size=batch_size)
    

    returns_cnn[i] = y_pred_test_cnn

# Plot predicted results
for i in outliers.columns.levels[0]:
    # Get predicted values


    
    
    # Plot actual vs. predicted values
    plt.figure(figsize=(12, 6))
    plt.plot(testset[i]["y"], label='Actual')
    plt.plot(returns_cnn[i], label='Predicted')
    plt.title('Actual vs. Predicted Close Price for ' + i)
    plt.xlabel('Time Step')
    plt.ylabel('Close Price')
    plt.legend()
    plt.show()

returns[i].shape

"""# Random Forest"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Create a Random Forest regressor model
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
histories_rf = {}
for i in outliers.columns.levels[0]:
    # Fit the model to the training data
    history_rf=rf_regressor.fit(trainset[i]["X"].reshape(-1, 5), trainset[i]["y"])
    histories_rf[i]=history_rf

# Predict the return for the test data
#y_pred = rf_regressor.predict(X_test)

# Predict the return for the test data

returns_rf = {}
# Loop through stocks
for i in outliers.columns.levels[0]:
    # Get predicted values
    y_pred_test_rf = rf_regressor.predict(testset[i]["X"].reshape(-1, 5))
    

    returns_rf[i] = y_pred_test_rf

# Plot predicted results
for i in outliers.columns.levels[0]:
    # Get predicted values


    
    
    # Plot actual vs. predicted values
    plt.figure(figsize=(12, 6))
    plt.plot(testset[i]["y"], label='Actual')
    plt.plot(returns_rf[i], label='Predicted')
    plt.title('Actual vs. Predicted Close Price for ' + i)
    plt.xlabel('Time Step')
    plt.ylabel('Close Price')
    plt.legend()
    plt.show()

"""## Models evaluation

#Linear Regression
"""

from sklearn.linear_model import LinearRegression

# Create a Linear Regression model
lr_model = LinearRegression()

# Create a Random Forest regressor model
histories_lr = {}
for i in outliers.columns.levels[0]:

    # Fit the model to the training data
    history_lr=lr_model.fit(trainset[i]["X"].reshape(-1, 5), trainset[i]["y"])
    histories_lr[i]=history_lr

# Predict the return for the test data

returns_lr = {}
# Loop through stocks
for i in outliers.columns.levels[0]:
    # Get predicted values
    y_pred_test_lr = lr_model.predict(testset[i]["X"].reshape(-1, 5))
    

    returns_lr[i] = y_pred_test_lr

# Plot predicted results
for i in outliers.columns.levels[0]:
    # Get predicted values


    
    
    # Plot actual vs. predicted values
    plt.figure(figsize=(12, 6))
    plt.plot(testset[i]["y"], label='Actual')
    plt.plot(returns_lr[i], label='Predicted')
    plt.title('Actual vs. Predicted Close Price for ' + i)
    plt.xlabel('Time Step')
    plt.ylabel('Close Price')
    plt.legend()
    plt.show()



"""#Evaluation"""

from sklearn.metrics import confusion_matrix

testset[i]["y"]

from sklearn.metrics import r2_score

# Evaluate the model on test data
lstm_mse = {}
cnn_mse = {}
rf_mse = {}
lr_mse = {}

r2_lstm={}
r2_cnn ={}
r2_rf = {}
r2_lr = {}
for i in outliers.columns.levels[0]:
    lstm_mse[i] = lstm.evaluate(testset[i]["X"], testset[i]["y"], batch_size=batch_size)
    cnn_mse[i] = model.evaluate(testset[i]["X"], testset[i]["y"], batch_size=batch_size)
    rf_mse[i] = mean_squared_error(testset[i]["y"],returns_rf[i])
    lr_mse[i] = mean_squared_error(testset[i]["y"],returns_lr[i])
    r2_lstm[i] = r2_score(testset[i]['y'], returns[i])
    r2_cnn[i] = r2_score(testset[i]['y'], returns_cnn[i])
    r2_rf[i] = r2_score(testset[i]['y'], returns_rf[i])
    r2_lr[i] = r2_score(testset[i]['y'], returns_lr[i])

# Convert the dictionaries to dataframes
mse_df = pd.DataFrame(list(lstm_mse.items()), columns=['Stock', 'LSTM'])
mse_df['CNN'] = list(cnn_mse.values())
mse_df['RF'] = list(rf_mse.values())
mse_df['LR'] = list(lr_mse.values())
r2_df = pd.DataFrame(list(r2_lstm.items()), columns=['Stock', 'LSTM'])
r2_df['CNN'] = list(r2_cnn.values())
r2_df['RF'] = list(r2_rf.values())
r2_df['LR'] = list(r2_lr.values())

mean_models = {}
std_models = {}
mean_r2 = {}
std_r2 = {}
for i in ['LSTM',	'CNN',	'RF',	'LR']:
  mean_models[i] = np.mean(mse_df[i])
  std_models[i] = np.std(mse_df[i])
  mean_r2[i] = np.mean(r2_df[i])
  std_r2[i] = np.std(r2_df[i])

# Define columns MultiIndex
columns = pd.MultiIndex.from_product([['Mean', 'Std'], ['MSE', 'R2']], 
                                     names=['Metric', 'Score'])

# Create DataFrame with the columns MultiIndex and the mean/std values
results_df = pd.DataFrame(index=mean_models.keys(), columns=columns)
for i in mean_models.keys():
    results_df.loc[i, ('Mean', 'MSE')] = mean_models[i]
    results_df.loc[i, ('Mean', 'R2')] = mean_r2[i]
    results_df.loc[i, ('Std', 'MSE')] = std_models[i]
    results_df.loc[i, ('Std', 'R2')] = std_r2[i]
    
# Reset index name
results_df.index.name = 'Model'

results_df

# Create an empty DataFrame to store average returns
av_ret = pd.DataFrame(columns=["Symbol","Average_Return"])

# Iterate over the stocks
for i in outliers.columns.levels[0]:
    # Calculate the average return for the current stock
    avg_return = np.mean(returns[i][-30:])
    # Create a temporary DataFrame with stock symbol and average return
    temp_df = pd.DataFrame({'Symbol': [i], 'Average_Return': [avg_return]})
    # Concatenate the temporary DataFrame to the main DataFrame
    av_ret = pd.concat([av_ret, temp_df], ignore_index=True)

# Sort the stocks based on average return in descending order
av_ret = av_ret.sort_values(by="Average_Return", ascending=False)

# Filter the DataFrame to keep only stocks with average returns greater than 0
positive_av_ret = av_ret[av_ret['Average_Return'] > 0]

# Merge the tables on "Symbol" column
merged_table = pd.merge(positive_av_ret, sp500, on='Symbol')

merged_table.head(20)

"""# Portfolio Optmization"""

my_portfolio = {}
for i in merged_table.Sector.unique():
  my_portfolio[i]=merged_table[merged_table.Sector == i].Symbol.head(5)
# Extract stock symbols from my_portfolio dictionary
stock_symbols = [symbol for symbols in my_portfolio.values() for symbol in symbols]

stock_symbols

my_stocks = {}
for x in stock_symbols:
  my_stocks[x] = returns[x]

from scipy.optimize import minimize

num_assets = len(my_stocks)
weights = np.ones(num_assets) / num_assets

returns_array = np.array(list(my_stocks.values())) # Convert dictionary values to NumPy array
stock_returns = returns_array.T
my_stocks_df=pd.DataFrame(stock_returns[0], columns =my_stocks.keys() )

# Function to calculate portfolio return
def portfolio_return(returns,weights):
    portfolio_return = np.sum([returns.tail(5).mean()[i] * weights[i] for i in range(len(my_stocks_df.columns))]) # Calculate weighted mean returns
    return portfolio_return

portfolio_return(my_stocks_df,weights)

# Function to calculate portfolio risk (variance)
def portfolio_risk(weights, returns):
    cov_matrix = np.cov(returns.tail(5).T) # Calculate covariance matrix
    return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) # Calculate portfolio risk

portfolio_risk(weights, my_stocks_df)

# Function for objective function in MVO
def objective_function(weights, returns, risk_aversion):
    return -portfolio_return(returns,weights) + risk_aversion * portfolio_risk(weights, returns)

objective_function(weights, my_stocks_df, 10)

# Function for MVO
def mean_variance_optimization(returns, risk_aversion):
    num_assets = returns.shape[1]
    initial_weights = np.ones(num_assets) / num_assets
    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
    bounds = [(0, None)] * num_assets
    result = minimize(objective_function, initial_weights, args=(returns, risk_aversion),
                      constraints=constraints, bounds=bounds, method='SLSQP')
    if result.success:
        optimal_weights = result.x
        return optimal_weights
    else:
        raise ValueError("Optimization failed. Please check inputs.")

# Example usage: calculate portfolio weights using MVO with risk aversion of 0.5
risk_aversion = 0.3
optimal_weights = mean_variance_optimization(my_stocks_df.tail(5), risk_aversion)
print("Optimal Portfolio Weights: ", optimal_weights)

portfolio_return(my_stocks_df,optimal_weights)

sum(optimal_weights)
